---
# This playbook will install Spark and user spark on every node in the cluster.
# If a node already has the particular version of Spark installed, the playbook will fail 
# for that node, but not the whole playbook.
# 
# Since, the playbook will only do an initial installation, it can be run at anytime, 
# such as when adding a new node.


########################################################################################
# start playbook
########################################################################################

- name: Deploy Spark {{ spark_version }}
  hosts: all
  remote_user: ansible
  become_user: root
  become: true
  become_method: sudo 
  connection: ssh
  gather_facts: no

  vars_files:
    - ../local_variable_files/spark-vars.yml


########################################################################################
# start tasks
########################################################################################

  tasks:
  - name: Gather facts about "{{ spark_home }}" and exit if "{{ spark_home }}" already exits
    stat:
      path: "{{ spark_home }}"
    register: p

  - fail:
      msg: "{{ spark_home }} already exists.  Run the delete script to remove Spark {{ spark_version }}, then rerun the deploy scipt."
    when: p.stat.isdir is defined and p.stat.isdir

  - name: Test for user {{ spark_user_name }} and create if not present
    getent:
      database: passwd
      key: "{{ spark_user_name }}"
      split: ':'
      fail_key: False

#  - debug: var=getent_passwd

  - debug:
      msg: "{{ spark_user_name }} is not present.  Create user {{ spark_user_name }} and group {{ spark_user_group }}"
    when: getent_passwd[ "{{ spark_user_name }}" ][4] is not defined

  - debug:
      msg: "{{ spark_user_name }} is already present."
    when: getent_passwd[ "{{ spark_user_name }}" ][4] is defined

  - name: Create group {{ spark_user_name }} if group {{ spark_user_name }} not present.  
    group:
      name: "{{ spark_user_name }}"
      state: present
    when: getent_passwd[ "{{ spark_user_name }}" ][4] is not defined
    become_user: root
    become: true
    become_method: sudo 

  - name: Create user {{ spark_user_name }} if not present
    user:
      name: "{{ spark_user_name }}"
      group: "{{ spark_user_name }}"
      password: "{{ spark_user_pwd | password_hash('sha512') }}"
      comment: User to run {{ spark_version }}
      shell: /bin/bash
      home: "{{ spark_user_home }}"
      update_password: on_create
    when: getent_passwd[ "{{ spark_user_name }}" ][4] is not defined
    become_user: root
    become: true
    become_method: sudo 

  - name: create {{ spark_base }}, if it does not exist
    file:
      path: "{{ spark_base }}"
      state: directory
      mode: 0755
      owner: "{{ spark_user_name }}"
      group: "{{ spark_user_group }}"

  - name: Extract {{ spark_compressed }}/{{ spark_file }} into {{ spark_base }}
    unarchive:
      src: "{{ spark_compressed }}/{{ spark_file }}"
      dest:  "{{ spark_base }}/"
      owner: "{{ spark_user_name }}"
      group: "{{ spark_user_group }}"
      mode: 0644

  - name: mv {{ spark_base }}/{{ spark_file_extracted }} to {{ spark_home }}
    command: mv {{ spark_base }}/{{ spark_file_extracted }} {{ spark_home }}

  - name: Ensure {{ spark_base }} directories are 0755
    command: find {{ spark_base }} -type d -exec chmod 0755 {} \;

  - name: Ensure {{ spark_base }} files are 0644
    command: find {{ spark_base }} -type f -exec chmod 0644 {} \;

  - name: Ensure {{ spark_home }}/bin files are 0755
    command: find {{ spark_home }}/bin -type f -exec chmod 0755 {} \;

  - name: Ensure {{ spark_home }}/sbin files are 0755
    command: find {{ spark_home }}/sbin -type f -exec chmod 0755 {} \;

# Note:
# although, openjdk-8-jdk is being referenced explicitly, it is possible to use statements such as
# sudo apt-get install packagename=version 
# to install the exact version of Java (7 or 8 for now), and the following will hold the version
# apt-mark hold <package-name>

  - name: Update repositories cache and install the openjdk-8-jdk package to remain on Java 8
    apt:
      name: openjdk-8-jdk
      update_cache: yes
      state: present

  - name: create {{ spark_defaults_conf_file_name }}
    copy:
      src:  "{{ spark_defaults_conf_file_path }}.template"
      dest: "{{ spark_defaults_conf_file_path }}"
      owner: "{{ spark_user_name }}"
      group: "{{ spark_user_group }}"
      mode: 0644

# iterate over the list and execute each element on the command line using the shell
  - name: set {{ spark_defaults_conf_file_name }}
    shell:  "{{ item }}" 
    with_items: "{{ spark_defaults_conf_file_list }}"

  - name: create {{ spark_env_file_name }}
    copy:
      src:  "{{ spark_env_file_path }}.template"
      dest: "{{ spark_env_file_path }}"
      owner: "{{ spark_user_name }}"
      group: "{{ spark_user_group }}"
      mode: 0755

# iterate over the list and execute each element on the command line using the shell
  - name: set {{ spark_env_file_name }}
    shell:  "{{ item }}" 
    with_items: "{{ spark_env_file_list }}"

  - name: Ensure {{ spark_user_name }} is owner of {{ spark_base }} and all contents
    file:
      path: "{{ spark_base }}"
      owner: "{{ spark_user_name }}"
      group: "{{ spark_user_group }}"
      recurse: yes 

