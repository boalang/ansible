# Purpose:
# This file contains variables needed to deploy and run Hadoop.

# note: hadoop_ver is passed in from the calling script
hadoop_version: "{{ hadoop_ver }}"

hadoop_base: /opt/hadoop
hadoop_prefix: "{{ hadoop_base }}/{{ hadoop_version }}"
hadoop_yarn_home: "{{ hadoop_prefix }}"

hadoop_file: hadoop-{{ hadoop_version }}.tar.gz
hadoop_file_checksum_mds: hadoop-{{ hadoop_version }}.tar.gz.mds
hadoop_compressed: /opt/compressed

hadoop_user_name:  hadoop
hadoop_user_group: "{{ hadoop_user_name }}"
hadoop_user_pwd:  "{{ hadoop_user_name }}"
hadoop_user_home: /home/{{ hadoop_user_name }}

hadoop_conf_dir: "{{ hadoop_base }}/{{ hadoop_version }}_conf_in_use"
hadoop_conf_master_dir: "{{ hadoop_base }}/{{ hadoop_version }}_conf_master"
hadoop_default_conf_dir:  etc/hadoop
hadoop_data_base_dir: /data

hadoop_env_file: "{{ hadoop_conf_dir }}/hadoop-env.sh"
hadoop_env_vars_profile_d_file:  /etc/profile.d/hadoop-{{ hadoop_version }}-env.sh

# these two directories only need to be on one drive, so they'll be coded to /data1, as it should always exist
hadoop_HADOOP_LOG_DIR: "{{ hadoop_data_base_dir }}1/{{ hadoop_version }}/logs"
hadoop_HADOOP_PID_DIR: "{{ hadoop_data_base_dir }}1/{{ hadoop_version }}/pids"
mapred_system_dir: /data1/{{ hadoop_version }}/mapred/system

hadoop_name_node: head
hadoop_secondary_name_node: head
hadoop_resourcemanager_node: head
hadoop_data_node_base_name: boa-
hadoop_num_data_node: 15
hadoop_num_drives_per_node: 2

path: "{{ hadoop_prefix }}/bin:{{ hadoop_prefix }}/sbin:{{ java_home }}/bin:{{ java_home }}/bin"

# hard-code java_home to the symlink java-1.8.0-openjdk-adm46
# to ensure the default-jdk doesn't inadvertenly update to java 9
java_home: /usr/lib/jvm/java-1.8.0-openjdk-amd64

hadoop_dfs_namenode_name_dir: name
hadoop_dfs_datanode_data_dir: hdfs-data
hadoop_dfs_namenode_checkpoint_dir: name-secondary
yarn_nodemanager_local_dirs: yarn-nodemanager-local-dir
mapreduce_cluster_local_dir: "mapred/local"
mapreduce_jobtracker_system_dir: "mapred/system"


# This dictionary will facilitate adding relevant export statements at the bottom of hadoop-env.sh
# When using the with_dict attribute with the file module, the module will automatically iterate over 
# all of the elements in the dictionary.

hadoop_env_list:
# java home
  - echo ""  >> {{ hadoop_env_file }};
  - echo \# Setting JAVA_HOME via Ansible playbook >> {{ hadoop_env_file }};
  - echo export JAVA_HOME={{ java_home }} >> {{ hadoop_env_file }}
# HADOOP_INSTALL=/opt/hadoop/version
  - echo "" >> {{ hadoop_env_file }};
  - echo \# Setting HADOOP_INSTALL via Ansible playbook >> {{ hadoop_env_file }};
  - echo export HADOOP_PREFIX={{ hadoop_prefix }} >> {{ hadoop_env_file }}
# HADOOP_CONF_DIR=/opt/hadoop/conf_version
  - echo ""  >> {{ hadoop_env_file }};
  - echo \# Setting HADOOP_CONF_DIR via Ansible playbook >> {{ hadoop_env_file }};
  - echo export HADOOP_CONF_DIR={{ hadoop_conf_dir }} >> {{ hadoop_env_file }}
# HADOOP_LOG_DIR=/data1/version/logs
  - echo ""  >> {{ hadoop_env_file }};
  - echo \# Setting HADOOP_LOG_DIR via Ansible playbook >> {{ hadoop_env_file }};
  - echo export HADOOP_LOG_DIR={{ hadoop_HADOOP_LOG_DIR }} >> {{ hadoop_env_file }}
# HADOOP_PID_DIR=/data1/version/pids
  - echo ""  >> {{ hadoop_env_file }};
  - echo \# Setting HADOOP_PID_DIR via Ansible playbook >> {{ hadoop_env_file }};
  - echo export HADOOP_PID_DIR={{ hadoop_HADOOP_PID_DIR }} >> {{ hadoop_env_file }}


protobuf_java_250_md5_name: protobuf-java-2.5.0.jar.md5
protobuf_java_250_md5_path: http://central.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/
protobuf_java_250_md5_url: http://central.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/{{ protobuf_java_250_md5_name }}
protobuf_java_250_jar_url: http://central.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar
